{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Plain\n",
      "Doctest mode is: ON\n"
     ]
    }
   ],
   "source": [
    "%doctest_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting misc.py\n"
     ]
    }
   ],
   "source": [
    "%%file misc.py\n",
    "### misc ###\n",
    "def get_two_senses_strs(record):\n",
    "    from Bio import SeqIO\n",
    "    \"\"\"\n",
    "    record: a SeqRecord object\n",
    "    returns a list with two strings: the sequence and the revcomp sequence\n",
    "    \"\"\"\n",
    "    return [str(record.seq), str(record.seq.reverse_complement())]\n",
    "\n",
    "def sed_subset_fastq_gz(full_filename, subset, subset_filename):\n",
    "    \n",
    "    values = [full_filename, subset*4, subset_filename]\n",
    "    \n",
    "    cline = \"zcat {0[0]} | sed -n 1,{0[1]}p | gzip > {0[2]}\".format(values)\n",
    "    \n",
    "    print \"A subset of %i sequences\"%(values[1]/4)\n",
    "    print \"from the file %s\"%values[0]\n",
    "    print \"will be written to %s\"%values[2]\n",
    "    print \"using the command\"\n",
    "    print cline\n",
    "    \n",
    "    err, out = execute_cline(cline)\n",
    "    \n",
    "    print \"stdout:\\n%s\"%out\n",
    "    print \"stderr:\\n%s\"%err\n",
    "    \n",
    "def parse_nhmmer_tblout(filname):\n",
    "    lines = [l for l in open(filname,'r').readlines() if not l.startswith('#')]\n",
    "    matches = []\n",
    "    for l in lines:\n",
    "        target_name,accession,query_name,accession,hmmfrom,\\\n",
    "        hmm_to,alifrom,ali_to,envfrom,env_to,sq_len,strand,\\\n",
    "        E_value,score,bias,description_of_target=l.rstrip().split()\n",
    "        matches.append({\n",
    "                'target_name':target_name,\n",
    "                'accession':accession,\n",
    "                'query_name':query_name,\n",
    "                'accession':accession,\n",
    "                'hmmfrom':hmmfrom,\n",
    "                'hmm_to':hmm_to,\n",
    "                'alifrom':alifrom,\n",
    "                'ali_to':ali_to,\n",
    "                'envfrom':envfrom,\n",
    "                'env_to':env_to,\n",
    "                'sq_len':sq_len,\n",
    "                'strand':strand,\n",
    "                'E_value':E_value,\n",
    "                'score':score,\n",
    "                'bias':bias,\n",
    "                'description_of_target':description_of_target\n",
    "                \n",
    "            })\n",
    "    return sorted(matches, key=lambda m: float(score), reverse=True)        \n",
    "\n",
    "def is_overlapping(coords1, coords2, max_overlapp=0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Check if two ranges are overlapping. The ranges are each\n",
    "    a list or tupple including of two values, the begining and end.\n",
    "    The begining and end are included in the range.\n",
    "    \n",
    "    # overlapping ranges\n",
    "    \n",
    "    >>> coords1 = (10, 100)\n",
    "    >>> coords2 = (50, 150)\n",
    "    >>> is_overlapping(coords1, coords2)\n",
    "    True\n",
    "    \n",
    "    >>> is_overlapping(coords2, coords1)\n",
    "    True\n",
    "    \n",
    "    # Second range is revcomp and input is\n",
    "    # list instead of tuple\n",
    "    \n",
    "    >>> coords1 = [10, 100]\n",
    "    >>> coords2 = [150, 50]\n",
    "    >>> is_overlapping(coords1, coords2)\n",
    "    True\n",
    "    \n",
    "    # One range is completely nested in the other\n",
    "    \n",
    "    >>> coords1 = (10, 100)\n",
    "    >>> coords2 = (20, 90)\n",
    "    >>> is_overlapping(coords1, coords2)\n",
    "    True\n",
    "    \n",
    "    >>> is_overlapping(coords2, coords1)\n",
    "    True\n",
    "    \n",
    "    # The ranges overlap by a single position\n",
    "    \n",
    "    >>> coords1 = (10, 100)\n",
    "    >>> coords2 = (100, 200)\n",
    "    >>> is_overlapping(coords1, coords2)\n",
    "    True\n",
    "    \n",
    "    >>> is_overlapping(coords2, coords1)\n",
    "    True\n",
    "    \n",
    "    # The ranges do not overlap\n",
    "    \n",
    "    >>> coords1 = (10, 100)\n",
    "    >>> coords2 = (200, 300)\n",
    "    >>> is_overlapping(coords1, coords2)\n",
    "    False\n",
    "    \"\"\"\n",
    "    \n",
    "    # inputs need to be lists or tuples\n",
    "    assert all([isinstance(coords1,(list,tuple)),\n",
    "                isinstance(coords2,(list,tuple))])\n",
    "    \n",
    "    for lst in [coords1, coords2]:\n",
    "        \n",
    "        # inputs need to have two values each\n",
    "        assert len(lst) == 2\n",
    "        \n",
    "        # inputs values have to be all int\n",
    "        for i in lst:\n",
    "            assert isinstance(i, int)\n",
    "    \n",
    "    if any([coords2[0] <= coords1[0] <= coords2[1]-max_overlapp,\n",
    "            coords2[0]+max_overlapp <= coords1[1] <= coords2[1],\n",
    "            coords1[0] <= coords2[0] <= coords1[1]-max_overlapp,\n",
    "            coords1[0]+max_overlapp <= coords2[1] <= coords1[1]]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "class Indel:\n",
    "    \n",
    "    def __init__(self, pos, length, indel_type, aln):\n",
    "        \n",
    "        \"\"\"\n",
    "        Characterizes indels in a top sequence alignment\n",
    "        compared to a bottom sequence alignment\n",
    "        pos: the position in the top sequence where the indel starts\n",
    "        length: the number of '-' characters following pos, either in\n",
    "        the top or in the bottom sequence\n",
    "        indel_type: insertion: the '-' characters are in the bottom \n",
    "        sequence. deletion: the '-' characters are in the top sequence.\n",
    "        aln: the alignment object.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.position = pos\n",
    "        self.length = length\n",
    "        self.type = indel_type\n",
    "        self.ref_alignment = aln\n",
    "        \n",
    "    def __str__(self):\n",
    "        return(\"start: %i,  length: %i, type: %s, sequence: %s, reference: %s\"%(\n",
    "                self.position,\n",
    "                self.length,\n",
    "                self.type,\n",
    "                self.ref_alignment[0].id,\n",
    "                self.ref_alignment[1].id\n",
    "               ))\n",
    "\n",
    "def is_indel(aln, i, indel_type):\n",
    "    k = None\n",
    "    if indel_type == 'insertion':\n",
    "        k = 1\n",
    "    elif indel_type == 'deletion':\n",
    "        k = 0\n",
    "    if aln[k,i] == '-' and (i == 0 or not aln[k,i-1] == '-'):\n",
    "        # find pos in top sequence\n",
    "        sub_top_seq_length=len(aln[0,:i])-str(aln[0,:i]).count('-')\n",
    "        # find insertion length (number of '-' in bottom sequence)\n",
    "        # or\n",
    "         # find deletion length (number of '-' in top sequence)\n",
    "        length = 0\n",
    "        j = i\n",
    "        p = aln[k,j]\n",
    "        while p == '-':\n",
    "            length += 1\n",
    "            j += 1\n",
    "            p = aln[k,j]\n",
    "        return Indel(sub_top_seq_length-1, length, indel_type, aln)\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "def is_insertion(aln, i):\n",
    "    return is_indel(aln, i, 'insertion')\n",
    "\n",
    "def is_deletion(aln, i):\n",
    "    return is_indel(aln, i, 'deletion')\n",
    "\n",
    "    \n",
    "def find_indels(aln):\n",
    "    indels = []\n",
    "    for i in range(aln.get_alignment_length()):\n",
    "        indel = is_insertion(aln, i)\n",
    "        if indel:\n",
    "            indels.append(indel)\n",
    "        else:\n",
    "            indel = is_deletion(aln, i)\n",
    "            if indel:\n",
    "                indels.append(indel)\n",
    "            \n",
    "    return indels\n",
    "\n",
    "\n",
    "def __read_kwargs__(suffix='.py', allowed=None, exe_path='',\n",
    "                    underscore_in_exe='-',\n",
    "                    underscore_in_keyword='-',\n",
    "                    **kwargs):\n",
    "    \"\"\"\n",
    "    Given a set of keyword arguments, format as commad line string\n",
    "    \n",
    "    # Make up a function with two allowed kwargs: a and bb\n",
    "    # It will write the function as a command line\n",
    "    \n",
    "    >>> def madeup(**kwargs):\n",
    "    ...     allowed = ['a','bb']\n",
    "    ...     cline = __read_kwargs__(allowed=allowed, **kwargs)\n",
    "    ...     return cline\n",
    "    \n",
    "    >>> print(madeup(a='a', bb='bb'))\n",
    "    madeup.py -a a --bb bb\n",
    "    \n",
    "    # Now try to use a keyword that is not allowed\n",
    "    >>> print(madeup(a='a', cc='cc'))\n",
    "    Traceback (most recent call last):\n",
    "     ...\n",
    "    IOError: Keyword cc is not allowed in madeup\n",
    "    \"\"\"\n",
    "    import inspect\n",
    "    curframe = inspect.currentframe()\n",
    "    calframe = inspect.getouterframes(curframe, 2)\n",
    "    caller = calframe[1][3]\n",
    "    \n",
    "    cline = ''\n",
    "    \n",
    "    if exe_path != '' and not exe_path.endwith('/'):\n",
    "        exe_path +- '/'\n",
    "    \n",
    "    u_in_e = underscore_in_exe\n",
    "    u_in_k = underscore_in_keyword\n",
    "    \n",
    "    for keyword in kwargs:\n",
    "        if allowed and not keyword in allowed:\n",
    "            raise IOError(\"Keyword %s is not allowed in %s\"%(keyword, caller))\n",
    "        prefix = '--'\n",
    "        if len(keyword) == 1:\n",
    "            prefix = '-'\n",
    "        if str(kwargs[keyword]) == 'True':\n",
    "            cline += \"%s%s \"%(prefix,\n",
    "                                 keyword.replace('_',u_in_k))\n",
    "        else:\n",
    "            cline += \"%s%s %s \"%(prefix,\n",
    "                                 keyword.replace('_',u_in_k),\n",
    "                                 str(kwargs[keyword]))\n",
    "    return \"%s%s%s %s\"%(exe_path,caller.replace('_',u_in_e),suffix,cline[:-1])\n",
    "\n",
    "def __sufix_kwargs__(cline, arg):\n",
    "    \"\"\"\n",
    "    Add arguments after the keyword arguments\n",
    "    \"\"\"\n",
    "    if type(arg) in [tuple,set,list]:\n",
    "        for a in arg:\n",
    "            cline += \" %s\"%a\n",
    "    else:\n",
    "        cline += \" %s\"%arg\n",
    "    return cline\n",
    "    \n",
    "    \n",
    "def __prefix_kwargs__(cline, arg):\n",
    "    \"\"\"\n",
    "    Add an arguments between the executable and the next argument/ kwarg\n",
    "    \"\"\"\n",
    "    if type(arg) in [tuple,set,list]:\n",
    "        raise IOError('Cannot prefix tuple, set ot list to cline')\n",
    "    cline = cline.partition(' ')\n",
    "    cline = \"%s %s %s\"%(cline[0],arg,cline[-1])\n",
    "    return cline\n",
    "\n",
    "def execute_cline(cline):\n",
    "    from subprocess import Popen,PIPE\n",
    "    p = Popen(cline, shell=True, stdout=PIPE, stderr=PIPE)\n",
    "    out, err = p.communicate()\n",
    "    return out, err\n",
    "\n",
    "def makedir(name, f=False):\n",
    "    import os\n",
    "    import warnings\n",
    "    if os.path.exists(name):\n",
    "        if f:\n",
    "            import shutil\n",
    "            shutil.rmtree(name)\n",
    "            os.mkdir(name)\n",
    "        else:\n",
    "            warnings.warn('keeping existing fpath %s'%name)\n",
    "    else:\n",
    "        os.mkdir(name)\n",
    "        warnings.warn('fpath %s newly created'%name)\n",
    "        \n",
    "def extractgz(source, destination):\n",
    "    \n",
    "    cline = 'zcat %s > %s'%(source, destination)\n",
    "    out, err = execute_cline(cline)\n",
    "    return out, err\n",
    "\n",
    "def compressgz(source):\n",
    "    \n",
    "    cline = 'gzip %s'%source\n",
    "    out, err = execute_cline(cline)\n",
    "    return out, err\n",
    "\n",
    "def pickledump(obj, target):\n",
    "    import cPickle as pickle\n",
    "    picout = open(target, 'wb')\n",
    "    pickle.dump(obj, picout, -1)\n",
    "    picout.close()\n",
    "\n",
    "def pickleload(source):\n",
    "    import cPickle as pickle\n",
    "    pkl_file = open(source, 'rb')\n",
    "    obj = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    return obj\n",
    "\n",
    "def printoe((out,err)):\n",
    "    if len(out) > 0:\n",
    "        print 'STDOUT:'\n",
    "        print out\n",
    "    if len(err) > 0:\n",
    "        print 'STDERR:'\n",
    "        print err\n",
    "        \n",
    "def extract_picload(source, destinationdir='./'):\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    if not destinationdir.endswith('/'):\n",
    "        destinationdir += '/'\n",
    "    pklfile = destinationdir+source.split('/')[-1].replace('.gz','')\n",
    "    printoe(\n",
    "        extractgz(source, pklfile)\n",
    "    )\n",
    "    \n",
    "    obj = pickleload(pklfile)\n",
    "    \n",
    "    os.remove(pklfile)\n",
    "    \n",
    "    return obj\n",
    "\n",
    "def picdump_compress(obj, destination):\n",
    "    import gc\n",
    "    \n",
    "    pickledump(obj, destination)\n",
    "    \n",
    "    printoe(\n",
    "        compressgz(destination)\n",
    "    )\n",
    "    \n",
    "    del obj\n",
    "    gc.collect()\n",
    "\n",
    "# Jellyfish\n",
    "def jellyfish_count(input_sequence_filenames = None,\n",
    "                    exe_path='',\n",
    "                    **kwargs):\n",
    "    \"\"\"\n",
    "    Count k-mers in fasta or fastq files\n",
    "\n",
    "    Options (default value in (), *required):\n",
    "     m, mer_len=int                     *Length of mer\n",
    "     s, size=string                     *Initial hash size (eg 100M)\n",
    "     t, threads=int                      Number of threads (1)\n",
    "     F, Files=int                        Number files open simultaneously (1)\n",
    "     g, generator=path                   File of commands generating fast[aq]\n",
    "     G, Generators=int                   Number of generators run simultaneously (1)\n",
    "     S, shell=string                     Shell used to run generator commands ($SHELL or /bin/sh)\n",
    "     o, output=string                    Output file (mer_counts.jf)\n",
    "     c, counter_len=Length in bits       Length bits of counting field (7)\n",
    "        out_counter_len=Length in bytes  Length in bytes of counter field in output (4)\n",
    "     C, canonical                        Count both strand, canonical representation (false)\n",
    "        bc=peath                         Bloom counter to filter out singleton mers\n",
    "        bf_size=string                   Use bloom filter to count high-frequency mers (eg 100M)\n",
    "        bf_fp=double                     False positive rate of bloom filter (0.01)\n",
    "        if=path                          Count only k-mers in this files\n",
    "     Q, min_qual_char=string             Any base with quality below this character is changed to N\n",
    "     p, reprobes=int                     Maximum number of reprobes (126)\n",
    "        text                             Dump in text format (false)\n",
    "        disk                             Disk operation. Do not do size doubling (false)\n",
    "     L, lower_count=string               Don't output k-mer with count < lower-count (eg 100M)\n",
    "     U, upper_count=string               Don't output k-mer with count > upper-count (eg 100M)\n",
    "        timing=Timing file               Print timing information\n",
    "        usage                            Usage\n",
    "     h, help                             This message (but for command line, not function)\n",
    "        full_help                        Detailed help\n",
    "     V, version                          Version\n",
    "     \n",
    "    # Test:\n",
    "    >>> jellyfish_count(m=21, s='100M', t=10, C=True)\n",
    "    Traceback (most recent call last):\n",
    "     ...\n",
    "    IOError: jellyfish_count: jellyfish count -C -m 21 -s 100M -t 10 : Must provide input_sequence_filenames as first arguments\n",
    "\n",
    "    \"\"\"\n",
    "    allowed = ['m', 'mer_len', 's', 'size', 't', 'threads', 'F', 'Files',\n",
    "              'g', 'generator', 'G', 'Generators', 'S', 'shell', 'o', 'output', \n",
    "              'out_counter_len','C', 'canonical','bc', 'bf_size', 'bf_fp', 'if',\n",
    "              'Q', 'min_qual_char', 'p', 'reprobes', 'text','disk','L', 'lower_count',\n",
    "              'U', 'upper_count', 'timing', 'usage', 'h', 'help', 'full_help','V', 'version'] \n",
    "    cline = __read_kwargs__(suffix='',\n",
    "                            exe_path=exe_path,\n",
    "                            allowed=allowed,\n",
    "                            underscore_in_exe=' ',\n",
    "                            underscore_in_keyword='-',\n",
    "                            **kwargs)\n",
    "    if input_sequence_filenames:\n",
    "        cline = __sufix_kwargs__(cline, input_sequence_filenames)\n",
    "        out, err = execute_cline(cline)\n",
    "        return out, err\n",
    "    else:\n",
    "        raise IOError(\"jellyfish_count: %s : Must provide input_sequence_filenames \"%cline+\n",
    "                      \"as first arguments\")\n",
    "\n",
    "def jellyfish_histo(kmer_hash_bin_filenames = None,\n",
    "                    exe_path='',\n",
    "                    **kwargs):\n",
    "    \"\"\"\n",
    "    Create an histogram of k-mer occurrences\n",
    "\n",
    "    Create an histogram with the number of k-mers having a given\n",
    "    count. In bucket 'i' are tallied the k-mers which have a count 'c'\n",
    "    satisfying 'low+i*inc <= c < low+(i+1)*inc'. Buckets in the output are\n",
    "    labeled by the low end point (low+i*inc).\n",
    "\n",
    "    The last bucket in the output behaves as a catchall: it tallies all\n",
    "    k-mers with a count greater or equal to the low end point of this\n",
    "    bucket.\n",
    "\n",
    "    Options (default value in (), *required):\n",
    "     l, low=int                         Low count value of histogram (1)\n",
    "     h, high=int                        High count value of histogram (10000)\n",
    "     i, increment=int                   Increment value for buckets (1)\n",
    "     t, threads=int                     Number of threads (1)\n",
    "     f, full                            Full histo. Don't skip count 0. (false)\n",
    "     o, output=string                   Output file\n",
    "     v, verbose                         Output information (false)\n",
    "     U, usage                           Usage\n",
    "        help                            This message\n",
    "        full_help                       Detailed help\n",
    "     V, version                         Version\n",
    "     \n",
    "    # Test:\n",
    "    >>> jellyfish_histo(full=True)\n",
    "    Traceback (most recent call last):\n",
    "     ...\n",
    "    IOError: jellyfish_histo: jellyfish histo --full : Must provide kmer_hash_bin_filenames as first arguments\n",
    "\n",
    "    \"\"\"\n",
    "    allowed = ['l', 'low', 'h', 'high', 'i', 'increment', 't',\n",
    "               'threads', 'f', 'full', 'o', 'output',\n",
    "               'v', 'verbose', 'U', 'usage', 'help',\n",
    "               'full_help', 'V', 'version'] \n",
    "    \n",
    "    cline = __read_kwargs__(suffix='',\n",
    "                            exe_path=exe_path,\n",
    "                            allowed=allowed,\n",
    "                            underscore_in_exe=' ',\n",
    "                            underscore_in_keyword='-',\n",
    "                            **kwargs)\n",
    "    if kmer_hash_bin_filenames:\n",
    "        cline = __sufix_kwargs__(cline, kmer_hash_bin_filenames)\n",
    "        out, err = execute_cline(cline)\n",
    "        return out, err\n",
    "    else:\n",
    "        raise IOError(\"jellyfish_histo: %s : Must provide kmer_hash_bin_filenames \"%cline+\n",
    "                      \"as first arguments\") \n",
    "        \n",
    "\n",
    "# Khmer\n",
    "def load_into_counting(output_countgraph_filename = None,\n",
    "                       input_sequence_filenames = None,\n",
    "                       **kwargs):\n",
    "    \"\"\"\n",
    "    Wrapper for load_into_counting.py\n",
    "    \n",
    "    Reuqired arguments:\n",
    "    output_countgraph_filename - filepath for the output\n",
    "    input_sequence_filenames   - a list of input fast[q/a] files\n",
    "    \n",
    "    optional arguments:\n",
    "    h, help             show this help message and exit\n",
    "    version             show program's version number and exit\n",
    "    ksize,  k\n",
    "                        k-mer size to use (default: 32)\n",
    "    n_tables, N\n",
    "                        number of tables to use in k-mer countgraph (default:\n",
    "                        4)\n",
    "    U, unique_kmers\n",
    "                        approximate number of unique kmers in the input set\n",
    "                        (default: 0)\n",
    "    fp_rate             Override the automatic FP rate setting for the current\n",
    "                        script (default: None)\n",
    "    max_tablesize, x\n",
    "                        upper bound on tablesize to use; overrides --max-\n",
    "                        memory-usage/-M. (default: 1000000.0)\n",
    "    M, max_memory_usage\n",
    "                        maximum amount of memory to use for data structure.\n",
    "                        (default: None)\n",
    "    threads, T\n",
    "                        Number of simultaneous threads to execute (default: 1)\n",
    "    b, no_bigcount      The default behaviour is to count past 255 using\n",
    "                        bigcount. This flag turns bigcount off, limiting\n",
    "                        counts to 255. (default: True)\n",
    "    summary_info, s\n",
    "                        What format should the machine readable run summary be\n",
    "                        in? (`json` or `tsv`, disabled by default) (default:\n",
    "                        None)\n",
    "    f, force            Overwrite output file if it exists (default: False)\n",
    "\n",
    "    Note: with `b`/`no_bigcount` the output will be the exact size of the k-mer\n",
    "    countgraph and this script will use a constant amount of memory. In exchange\n",
    "    k-mer counts will stop at 255. The memory usage of this script with `b` will\n",
    "    be about 1.15x the product of the `x` and `N` numbers.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        load_into_counting('out', 'data/100k-filtered.fa', k=20 x=5e7)\n",
    "\n",
    "    Multiple threads can be used to accelerate the process, if you have extra cores\n",
    "    to spare.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        load_into_counting('out', 'data/100k-filtered.fa', k=20, x=5e7, T=4)\n",
    "   \n",
    "    # Test:\n",
    "    >>> load_into_counting(k=20, x=5e7, T=4)\n",
    "    Traceback (most recent call last):\n",
    "     ...\n",
    "    IOError: load_into_counting: load-into-counting.py -x 50000000.0 -k 20 -T 4 : Must provide output_countgraph_filename and input_sequence_filenames as first arguments\n",
    "    \"\"\"\n",
    "    \n",
    "    allowed = ['ksize',  'k', 'h', 'help', 'version', 'n_tables', 'N',\n",
    "               'U', 'unique_kmers', 'fp_rate', 'max_tablesize', 'x',\n",
    "               'M', 'max_memory_usage', 'threads', 'T', 'b', 'no_bigcount',\n",
    "               'summary_info', 's', 'f', 'force']\n",
    "    \n",
    "    cline = __read_kwargs__(suffix='.py', allowed=allowed, **kwargs)\n",
    "    \n",
    "    if output_countgraph_filename and input_sequence_filenames:\n",
    "        if not isinstance(input_sequence_filenames,list):\n",
    "            raise IOError(\"input_sequence_filenames must be list\")\n",
    "        cline = __sufix_kwargs__(cline, output_countgraph_filename)\n",
    "        cline = __sufix_kwargs__(cline, input_sequence_filenames)\n",
    "        out, err = execute_cline(cline)\n",
    "        return out, err\n",
    "        \n",
    "    else:\n",
    "        raise IOError(\"load_into_counting: %s : Must provide output_countgraph_\"%cline+\n",
    "                      \"filename and input_sequence_filenames \"+\n",
    "                      \"as first arguments\")\n",
    "        \n",
    "def iupac(pos):\n",
    "    \n",
    "    from collections import Counter\n",
    "    \n",
    "    if len(pos) > 4:\n",
    "        raise IOError('up to four pos allowed for IUPAC')\n",
    "        \n",
    "    if any([v > 1 for v in Counter(pos).itervalues()]):\n",
    "        raise IOError('pos cannot repeat in IUPAC')\n",
    "        \n",
    "    allowed = 'atgcATGC'\n",
    "    \n",
    "    if any([p not in allowed for p in pos]):\n",
    "        raise IOError('only '+str(allowed)+ ' allowed in IUPAC')\n",
    "        \n",
    "    pos = ''.join(sorted(''.join(pos))).upper()\n",
    "        \n",
    "    \n",
    "    IUPAC = { 'AG':'R',\n",
    "              'GT':'K',\n",
    "              'AC':'M',\n",
    "              'AT':'W',\n",
    "              'CT':'Y',\n",
    "              'CG':'S',\n",
    "              'CGT':'B',\n",
    "              'ACG':'V',\n",
    "              'AGT':'D',\n",
    "              'ACT':'H',\n",
    "              'ACGT':'N',\n",
    "              'A': 'A',\n",
    "              'T': 'T',\n",
    "              'G': 'G',\n",
    "              'C': 'C'\n",
    "    }\n",
    "    \n",
    "    \n",
    "    return IUPAC[pos]\n",
    "\n",
    "def mafft_align_contigs(fpath):\n",
    "    \n",
    "    from Bio import AlignIO\n",
    "    import subprocess\n",
    "    from Bio.Align.Applications import MafftCommandline\n",
    "    import warnings\n",
    "    \n",
    "    # Align the sequences\n",
    "    mafft_cline = MafftCommandline(input=fpath, op=3.0)\n",
    "    child = subprocess.Popen(str(mafft_cline),\n",
    "                             stdout=subprocess.PIPE,\n",
    "                             stderr=subprocess.PIPE,\n",
    "                             shell=True)\n",
    "    \n",
    "    align = AlignIO.read(child.stdout, \"fasta\")\n",
    "    \n",
    "    if len(align[0].seq) == str(align[0].seq).count('-'):\n",
    "        warnings.warn('bad alignment pair %s'%input)\n",
    "\n",
    "    if len(align[1].seq) == str(align[1].seq).count('-'):\n",
    "        warnings.warn('bad alignment pair %s'%input)\n",
    "    \n",
    "    return align\n",
    "\n",
    "def get_aln_cumulative_entropy(align, char_type = 'dna'):\n",
    "    # get cumulative entropy for alignment\n",
    "    from reprophylo import entropy\n",
    "\n",
    "    entropies =[]\n",
    "    for i in range(align.get_alignment_length()):\n",
    "        column = align[:,i]\n",
    "        entropies.append(entropy(column, char_type))\n",
    "    cum_entropy = sum(entropies) \n",
    "    #print \"Entropies:\", str(entropies[:10]),'... Total:', cum_entropy\n",
    "    return cum_entropy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!git add --all ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master c6ba3d9] seq comparison\r\n",
      " 2 files changed, 94 insertions(+), 1 deletion(-)\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"seq comparison\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
